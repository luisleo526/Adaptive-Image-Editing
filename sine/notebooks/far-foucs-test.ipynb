{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--prompt\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=\"a painting of a virus monster playing guitar\",\n",
    "        help=\"the prompt to render\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--outdir\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        help=\"dir to write results to\",\n",
    "        default=\"outputs/txt2img-samples\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_grid\",\n",
    "        action='store_true',\n",
    "        help=\"do not save a grid, only individual samples. Helpful when evaluating lots of samples\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_save\",\n",
    "        action='store_true',\n",
    "        help=\"do not save individual samples. For speed measurements.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ddim_steps\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"number of ddim sampling steps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plms\",\n",
    "        action='store_true',\n",
    "        help=\"use plms sampling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--laion400m\",\n",
    "        action='store_true',\n",
    "        help=\"uses the LAION400M model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fixed_code\",\n",
    "        action='store_true',\n",
    "        help=\"if enabled, uses the same starting code across samples \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ddim_eta\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_iter\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"sample this often\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--H\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image height, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--W\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image width, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--C\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"latent channels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--f\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"downsampling factor\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"how many samples to produce for each given prompt. A.k.a. batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_rows\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"rows in the grid (default: n_samples)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale\",\n",
    "        type=float,\n",
    "        default=7.5,\n",
    "        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--from-file\",\n",
    "        type=str,\n",
    "        help=\"if specified, load prompts from this file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"configs/stable-diffusion/v1-inference.yaml\",\n",
    "        help=\"path to config which constructs model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt\",\n",
    "        type=str,\n",
    "        default=\"models/ldm/stable-diffusion-v4/sd-v1-4-full-ema.ckpt\",\n",
    "        help=\"path to checkpoint of model\",\n",
    "    )   \n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--sin_config\",\n",
    "        type=str,\n",
    "        default=\"configs/stable-diffusion/v1-inference.yaml\",\n",
    "        nargs='+'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sin_ckpt\",\n",
    "        type=str,\n",
    "        nargs='+'\n",
    "    )\n",
    "     \n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help=\"the seed (for reproducible sampling)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        choices=[\"full\", \"autocast\"],\n",
    "        default=\"full\"\n",
    "    )\n",
    "    parser.add_argument(\"--single_guidance\", action=\"store_true\")\n",
    "    parser.add_argument(\"--range_t_max\", type=int, default=400, nargs='+')\n",
    "    parser.add_argument(\"--range_t_min\", type=int, default=1, nargs='+')\n",
    "    parser.add_argument(\"--beta\", type=float, default=0.5, nargs='+')\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--embedding_path\", \n",
    "        type=str, \n",
    "        help=\"Path to a pre-trained embedding manager checkpoint\")\n",
    "    \n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_args()\n",
    "opt.single_guidance=True\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "LOG_DIR=\"./logs/FarFocus\"\n",
    "opt.sin_ckpt=[ LOG_DIR+\"_Train/checkpoints/last.ckpt\", LOG_DIR+\"_Test/checkpoints/last.ckpt\" ]\n",
    "opt.sin_config = ['./configs/stable-diffusion/v1-inference.yaml', './configs/stable-diffusion/v1-inference.yaml']\n",
    "opt.ckpt = './models/pretrained.ckpt'\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "#model.embedding_manager.load(opt.embedding_path)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "guidance_configs = opt.sin_config\n",
    "guidance_ckpts = opt.sin_ckpt\n",
    "\n",
    "assert len(guidance_configs) == len(guidance_ckpts)\n",
    "\n",
    "num_guidance = len(guidance_configs)\n",
    "guidance_models = [load_model_from_config(OmegaConf.load(config), ckpt).to(device) for config, ckpt in zip(guidance_configs, guidance_ckpts)]\n",
    "\n",
    "from ldm.models.diffusion.guidance_ddim import DDIMMultiSampler\n",
    "sampler = DDIMMultiSampler(model, guidance_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(opt, beta, tmax, tmin, ddim_eta, ddim_steps, scale):\n",
    "\n",
    "    opt.beta = beta\n",
    "    opt.range_t_max = tmax\n",
    "    opt.range_t_min = tmin\n",
    "    opt.ddim_eta = ddim_eta\n",
    "    opt.scale = scale\n",
    "    opt.ddim_steps = ddim_steps\n",
    "\n",
    "    guidance_betas = opt.beta if isinstance(opt.beta, list) else [opt.beta]*num_guidance\n",
    "    guidance_range_t_max = opt.range_t_max if isinstance(opt.range_t_max, list) else [opt.range_t_max]*num_guidance\n",
    "    guidance_range_t_min = opt.range_t_min if isinstance(opt.range_t_min, list) else [opt.range_t_min]*num_guidance\n",
    "    assert len(guidance_betas) == num_guidance and len(guidance_range_t_max) == num_guidance and len(guidance_range_t_min) == num_guidance\n",
    "\n",
    "    for sin_model, beta, t_max, t_min in zip(sampler.guide_model_list, guidance_betas, guidance_range_t_max, guidance_range_t_min):\n",
    "        sin_model.extra_config = {\"beta\": beta, \"range_t_max\": t_max, \"range_t_min\": t_min}\n",
    "\n",
    "    prompt=opt.prompt\n",
    "\n",
    "    start_code = None\n",
    "    if opt.fixed_code:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                for sin_model in guidance_models:\n",
    "                    sin_model.ema_scope()\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                for n in range(opt.n_iter):\n",
    "                    for prompts in data:\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                            uc_sin_list = [sin_model.get_learned_conditioning(batch_size * [\"\"]) for sin_model in guidance_models]\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "\n",
    "                        if opt.single_guidance:\n",
    "                            b = len(prompts)\n",
    "                            prompt = prompts[0]\n",
    "                            prompts = [prompt.split('[SEP]')[0].strip()] * b\n",
    "\n",
    "                            prompts_single = [[p.strip()] * b for p in prompt.split('[SEP]')[1:]]\n",
    "\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "                        c_sin_list = [sin_model.get_learned_conditioning(p) for sin_model, p in zip(guidance_models, prompts_single)]\n",
    "\n",
    "                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        conditioning_single_list=c_sin_list,\n",
    "                                                        batch_size=opt.n_samples,\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=opt.scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        unconditional_conditioning_single_list=uc_sin_list,\n",
    "                                                        eta=opt.ddim_eta,\n",
    "                                                        x_T=start_code)\n",
    "\n",
    "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                        if not opt.skip_grid:\n",
    "                            all_samples.append(x_samples_ddim)\n",
    "\n",
    "                if not opt.skip_grid:\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=opt.n_rows)\n",
    "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                    img=Image.fromarray(grid.astype(np.uint8))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.prompt=\"A photo of a dog sitting in the library[SEP]\"+\\\n",
    "           \"A far foucus camera photo of a dog sitting in the library[SEP]\"+\\\n",
    "           \"A photo of a dog sitting in the library\"\n",
    "opt.n_iter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.guide_model_list = guidance_models\n",
    "sampler.model = model\n",
    "# Results of method 2\n",
    "inference(opt=opt, beta=[0.2, 0.7], tmax=[1000,1000], tmin=[400,800], ddim_eta=0.0, ddim_steps=100, scale=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.guide_model_list = [guidance_models[1], guidance_models[1]]\n",
    "opt.n_samples = 4\n",
    "# Results of SINE\n",
    "inference(opt=opt, beta=[0.2, 0.7], tmax=[1000,1000], tmin=[400,800], ddim_eta=0.0, ddim_steps=100, scale=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-complement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
